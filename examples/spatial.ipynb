{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Inversion of dv/v\n",
    "This is the second notebook in the SeisMIC tutorial series. if you have not looked at the first notebook yet, we strongly comment starting [there](./tutorial.ipynb).\n",
    "\n",
    "\n",
    "SeisMIC can invert dv/v time series from several station combination onto a spatial grid using the method proposed by [Obermann, et al. (2013)](https://doi.org/10.1002/2013JB010399). This method is based on 2D sensitivity kernels that arise from the time-dependent solution of the Boltzmann equation for a homogeneous medium (see [Paasschens, 1997](https://doi.org/10.1103/PhysRevE.56.1135)).\n",
    "A demonstration with real data would require a bit too much download and a very dense network.\n",
    "\n",
    "So for this demonstration we commit a little \"inversion crime\" and try to re-obtain a velocity change model that is already known to us.\n",
    "\n",
    "**Note that to execute this tutorial, you will have to install jupyter into your environment (e.g. via `pip install jupyer`).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the parameters\n",
    "We use the class `seismic.monitor.spatial.DVGrid` to initialise an empty grid, for which we compute the sensitivity kernels, the forward model, and the inverse model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seismic.monitor.spatial import DVGrid\n",
    "\n",
    "# For convenience, we define the map at the equator\n",
    "lat0 = lon0 = 0  # lower left corner\n",
    "\n",
    "# Y- and X-extent in km\n",
    "y = x = 40\n",
    "res = 1  # resolution in km\n",
    "vel = 3  # wave velocity km/s\n",
    "mf_path = 30  # mean free path in km, dependent on the geology\n",
    "dt = 0.05  # Sampling interval to compute the sensitivity kernels\n",
    "# dt should be larger than res/vel. If not, the kernels will be aliased.\n",
    "# Small dt makes a smoother kernel, but the computation takes longer.\n",
    "\n",
    "# Initialise the grid\n",
    "dvg = DVGrid(lat0, lon0, res, x, y, dt, vel, mf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a synthetic velocity grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from obspy import UTCDateTime\n",
    "\n",
    "from seismic.plot.plot_utils import set_mpl_params\n",
    "\n",
    "chkb = np.zeros_like(dvg.xgrid)\n",
    "for ii, yy in enumerate(np.arange(y/res+1)):\n",
    "    chkb[ii, :] = np.sin(\n",
    "        2*np.pi*np.arange(x/res+1)/(x/res)) + np.cos(2*np.pi*yy/(y/res))\n",
    "chkb /= 100\n",
    "\n",
    "# Plot the synthetic checkerboard\n",
    "set_mpl_params()\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(chkb*-100, cmap='seismic_r')\n",
    "plt.colorbar(label='Velocity perturbation (%)')\n",
    "plt.title('Synthetic Velocity-Change Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define station locations and compute dv/v values for each combination\n",
    "\n",
    "Now we will place stations randomly onto our grid and use a forward modelling approach to\n",
    "compute the velocity change that would be sensed between each station pair (i.e., now we will look at \"cross-correlations\").\n",
    "\n",
    "Feel free to change the number of stations `nsta` to see by how much we can increase the accuracy of our inversion result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from seismic.monitor.dv import DV\n",
    "from seismic.correlate.stats import CorrStats\n",
    "\n",
    "### parameters ###\n",
    "#  #### You can play with these parameters to see how it affects the result\n",
    "nsta = 2  # number of stations, increases computation time\n",
    "\n",
    "# lapse time parameters in s\n",
    "# don't set these to small (have to contain the ballistic wave arriving\n",
    "# with the velocity of the medium)\n",
    "tw_start = 10\n",
    "tw_len = 17\n",
    "\n",
    "# frequency band\n",
    "freq_min = .5\n",
    "freq_max = 1.5\n",
    "\n",
    "# Should the lapse time window be split up into several subwindows\n",
    "# Note that this could increase the spatial sensitivity. But obviously\n",
    "# also increases the computation time\n",
    "n_split = 1  # if n_split > 1, tw_len has to be divided by n_split\n",
    "\n",
    "use_auto_correlations = True  # use auto correlations\n",
    "use_cross_correlations = True  # use cross correlations\n",
    "\n",
    "#  #### end of parameters ####\n",
    "\n",
    "\n",
    "# #### actual code ####\n",
    "# distribute stations randomly\n",
    "np.random.seed(1234)\n",
    "sta_x = np.random.random(nsta)*x\n",
    "sta_y = np.random.random(nsta)*y\n",
    "\n",
    "# convert into lat/lon not 100% but good enough as close to equator\n",
    "sta_lon = sta_x/111.19492664455873\n",
    "sta_lat = sta_y/111.19492664455873\n",
    "\n",
    "dvs = []\n",
    "tw_len = tw_len//n_split\n",
    "for slat0, slon0 in zip(sta_lat, sta_lon):\n",
    "    for slat1, slon1 in zip(sta_lat, sta_lon):\n",
    "        if slat0 == slat1 and slon0 == slon1 and not use_auto_correlations:\n",
    "            # we don't want auto correlations\n",
    "            continue\n",
    "        if any(\n",
    "            [slat0 != slat1, slon0 != slon1])\\\n",
    "                and not use_cross_correlations:\n",
    "            # we don't want cross correlations\n",
    "            continue\n",
    "\n",
    "        # create dv.stats\n",
    "        cst = CorrStats(\n",
    "            {\n",
    "                'stla': slat0, 'stlo': slon0, 'evla': slat1, 'evlo': slon1,\n",
    "                'corr_start': np.array(\n",
    "                    [UTCDateTime(ii*3600) for ii in np.arange(35)]),\n",
    "                'corr_end': np.array(\n",
    "                    [UTCDateTime((ii+1)*3600) for ii in np.arange(35)])})\n",
    "        for ii in range(n_split):\n",
    "            dv = DV(\n",
    "                .9*np.ones((35, )), np.ones((35, )), 'stretch', None,\n",
    "                None, 'modelled', cst,\n",
    "                dv_processing={\n",
    "                    'tw_start': tw_start + ii*tw_len, 'tw_len': tw_len,\n",
    "                    'freq_min': freq_min, 'freq_max': freq_max})\n",
    "            dvs.append(dv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward modelling\n",
    "Depending on your choice of parameters, this could take a little while.\n",
    "This will compute and cache the sensitivity kernels, so if you add more stations\n",
    "to the list afterwards, only new sensitivity kernels will be computed. All the parameters\n",
    "will be extracted from the `DV` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_model = dvg.forward_model(\n",
    "    chkb, dvs=dvs, utc=dvs[0].stats.corr_start[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add random noise\n",
    "add random noise to the forward model. You can play with the amplitude here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set standard deviation of the noise from zero\n",
    "noise_amp = 1e-3\n",
    "\n",
    "# add random noise\n",
    "# assign values. Technicall we should only set the value at index 5\n",
    "# but it doesn't really matter so much\n",
    "gen = np.random.Generator(np.random.PCG64(1234))\n",
    "for dv, fwd_val in zip(dvs, fwd_model):\n",
    "    dv.value[:] = fwd_val\n",
    "    dv.value += gen.normal(size=len(dv.value), scale=noise_amp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot a sensitivity kernel\n",
    "Let's have a look at one of the kernels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(12, 9))\n",
    "surf = ax.plot_surface(dvg.xgrid, dvg.ygrid, np.reshape(list(dvg.skernels.values())[-2], dvg.xgrid.shape), linewidth=0, antialiased=True, cmap='viridis')\n",
    "plt.colorbar(surf);\n",
    "plt.xlabel('x (km)')\n",
    "plt.ylabel('y (km)')\n",
    "plt.title('An arbitrary sensitivity kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute inverse model\n",
    "For the inverse model, we will have to set damping parameters. These parameters\n",
    "are the `corr_len` that basically smoothes the final grid output to avoid sparsities.\n",
    "And `std_model`, which sets the allowed standard deviation of the output model (i.e.,\n",
    "limits the search radius around a starting model=0). See Obermann et a., 2013 for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothing parameters\n",
    "corr_len = 3  # correlation length in km\n",
    "std_model = 0.05  # standard deviation of the model\n",
    "\n",
    "inverse_model = dvg.compute_dv_grid(\n",
    "    dvs, dvs[0].stats.corr_start[5], scaling_factor=res, corr_len=corr_len,\n",
    "    std_model=std_model)\n",
    "\n",
    "# Let's have a first look at the result\n",
    "dvg.plot()\n",
    "plt.title('Inversion result');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a comparison between synthetic and inverse\n",
    "\n",
    "You will see that increasing the number of stations, will make the biggest difference to the accuracy of your inversion results. However, also using several time windows and both auto and cross-correlations will make an impact.\n",
    "However, it is important to choose the correct damping parameters. usually, these are determined via an L-curve criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_mpl_params()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "\n",
    "dvg.plot(ax=ax1)\n",
    "ax2 = plt.subplot(1, 2, 2, sharey=ax1)\n",
    "\n",
    "# norm = mpl.colors.TwoSlopeNorm(vcenter=0)\n",
    "map2 = ax2.imshow(\n",
    "    -np.flipud(chkb)*100, cmap='seismic_r',\n",
    "    extent=[dvg.xaxis.min(), dvg.xaxis.max(), dvg.yaxis.min(), dvg.yaxis.max()]);\n",
    "plt.scatter(dvg.statx, dvg.staty, s=30, c='k', edgecolors='white', marker='v')\n",
    "plt.xlabel('Easting [km]')\n",
    "plt.ylabel('Northing [km]')\n",
    "plt.colorbar(map2, orientation='horizontal', label=r'$\\frac{dv}{v}$ [%]')\n",
    "\n",
    "ax1.set_title('Inversion result');\n",
    "ax2.set_title('Synthetic Velocity-Change Model');\n",
    "\n",
    "plt.suptitle('Comparison of the inversion result with the synthetic model');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[Obermann, A., Planès, T., Larose, E., and Campillo, M. (2013), Imaging preeruptive and coeruptive structural and mechanical changes of a volcano with ambient seismic noise, J. Geophys. Res. Solid Earth, 118, 6285– 6294.](https://doi.org/10.1002/2013JB010399)\n",
    "\n",
    "[Paasschens, J. C. J. \"Solution of the time-dependent Boltzmann equation.\" Physical Review E 56.1 (1997): 1135.](https://doi.org/10.1103/PhysRevE.56.1135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download this notebook\n",
    "You can download this notebook on our [GitHub page](https://github.com/PeterMakus/SeisMIC/blob/main/examples/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
